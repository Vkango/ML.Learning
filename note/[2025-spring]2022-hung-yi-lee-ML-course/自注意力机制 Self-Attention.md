# 自注意力机制 Self-Attention

## 总结

### 适用范围

样本输入比较复杂：长sequence

举例：句子、语音

### 问题提出

**复杂**的输入信息的**上下文**中会存在**关系**。Attention层就是让**输入向量**中各个元素了解彼此的关系。

### 参数列表 | 3个需要训练的参数矩阵

$W_q$：query权重矩阵

$W_k$：key权重矩阵

$W_v$：vector权重矩阵

### 运算方式

1. 三个参数矩阵与**输入矩阵**$I$相乘，得到$Q,K,V$三个矩阵。

   结果：得到`query`、`key`、`vector`向量，用于计算`关联度`

   词与词的关联度：当前单词的`query` $·(点积)$ 与其他单词的`key`

2. 激活函数归一化，得到关联度值。关联度值越大，提起信息中加权求和权重越大。

3. 最终注意力权重计算公式：$A'=softmax(K·Q)$

4. 抽取后的输出需要再乘以`vector`矩阵，为$O=VA'$

结果举例：it做代词代表的名词，那么it与这个名词对应的**关联度**应该最大。

### $W_Q$、$W_K$、$W_V$

#### 名词解释

> [!TIP]
>
> **关注程度** 和 **关联程度** 概念上互通。

- **Query（查询向量）**：用于表示当前元素**想要寻找**的信息。表达当前词对其他词的关注点。
  - 表达了**当前元素**对其他元素的兴趣点。`学生向老师问的具体问题`
  - 例如：`cat` 这个词的 `query` 向量可以表达它**想**找到与**动物**相关的信息。再与 `key` 向量进行比较，来决定其他元素于自己的相关性。

- **Key（键向量）**：用于表示每个元素**可以提供**哪些信息。Key 向量是其他元素（通过 Query 向量）可以匹配的对象。
  - 表达了每个元素**能够提供**的信息。 `老师知道的知识的名字`
  - 例如：`run` 这个词的 `key` 向量可以表达它与 `动作` 相关的信息。
  - `key` 向量 与 `query` 向量进行比较来决定自己与 `query` 的相关性。
- **Value（值向量）**：表示每个元素的**实际内容**。当 Query 和 Key 匹配成功后，Value 就是最终被加权使用的部分。也就是说 Value 是 Query 和 Key 再综合学习后的产物。 `知道知识的具体内容`
  - 实际信息：Query 和 Key 匹配成功后，Value 就是最终被加权使用的部分。
  - `cat` 这个词的 `value` 向量可以包含它的 `词义` `语义特征` 等。
  - Value 向量根据注意力权重被加权求和，从而生成最终输出。

> [!WARNING]
>
> 这三个向量可以说是独立存在的，因为他们是通过各自不同的线性变换得到的。表示的意义也不相同。
>
> 也可以说是相互联系的，因为可以通过`Query`和`Key`向量计算注意力权重，再对`Value`向量进行加权求和。

#### 举例

`猫` `在` `草地上` `跑` 

`猫` 提出了一个问题 `query`：想知道自己相关的信息，如：`动作`、`地点` 等。

`跑` 的 `key` 是一个知识库：知道自己是一个 `动作` ，并且与 `动物` 相关(动物发出的动作)……

##### 计算注意力权重

计算注意力权重：`猫` 的 `query` 向量 与 `跑` 的 `key` 向量进行 `点积` 运算，得到注意力分数。这个值反映了**要查的内容，与我知道的相关性大不大。越大那我越想回答。越不需要的，我不用给太多注意力。**

假设得到了注意力权重：

$a_{1,1}=0.1$ `猫` 对自己的注意力权重。

$a_{1,2}=0.2$ `猫` 对 `在` 的注意力权重。

$a_{1,3}=0.3$ `猫` 对 `草地上` 的注意力权重。

$a_{1,4}=0.4$ `猫` 对 `跑` 的注意力权重。

……

##### 利用注意力权重对 `Value` 向量进行加权求和

假设各个词的 `Value` 向量如下：

设 `Value` 向量三个成员意义分别为：`[动作相关性, 位置相关性, 主体相关性]`

$v_1=[0,0,1]$ (`猫`是一个主体)

$v_2=[0,1,0]$ (`在` 一般与`具体位置`一起出现，所以说与位置有关)

$v_3=[0,1,0]$ (`草地上` 是一个位置)

$v_4=[1,0,0]$ (`跑` 是一个动作)

加权求和：利用注意力权重，`猫` 的输出表示会包含 `跑` 的 Value 向量的**一部分** (满足最需要的即可，越不需要的，所占比重越小)，这样就表达了 `猫` 与 `跑` 的语义关系。

## 正文

### 解决什么问题 / 输出什么

解决复杂input问题，vector数量可能不止一个或者不确定。

举例：句子、声音、图。

输出：

1. m2m输入向量都有一个label，输入输出数目一样。
2. m2one多输入单输出。
3. s2s输出的长度不一定(翻译)。

### 多输入多输出任务

输入的句子长度不一定。

解决方案1. window长度固定，提供上下文信息。

缺点：句子长度不一定，window长度难以把握。

解决方案2. self-attention

衡量**输入向量**之间的`关联度`$\alpha$

产生输出$b_1$，这个$b_1$用于衡量关注程度。

> [!NOTE]
>
> **举例** 输入向量为$a^1,a^2,a^3,a^4$，其中
>
> - $a_1=[0.1,0.2,0.3]$（“猫”）
> - $a_2=[0.4,0.5,0.6]$（“在”）
> - $a_3=[0.7,0.8,0.9]$（“草地上”）
> - $a_4=[1.0,1.1,1.2]$（“跑”）
>
> 并不是一定要三个维度，词嵌入的维度是一个超参数，因模型而异。
>
> 可确认多个，计算方法为`Dot-product`方法。
>
> 设**当前输入向量**为 `query` ，它对应有权重矩阵 $W^q$，`query` 的值为**权重矩阵**乘以**输入向量**。而`query`的输入向量是`key`，对应有权重矩阵$W_k$，`key`的值为权重矩阵乘以输入矩阵。
>
> $W^q,W^k,W^v$的矩阵大小
>
> $q^i=W^q·a^i,k^i=W^k·a^i$
>
> 得到$\alpha_{i,j}$之后使用激活函数激活，输出激活后的$\alpha'$，即需要的注意力权重。
>
> `value` 计算
>
> 求出注意力权重后，计算$v^i=W^V·\alpha^i$。

有了以上参数，可求得

> $$
> b^1=\sum_i\alpha_{1,i}'v^i$
> $$

以上参数可并行计算。$q^i,k^i,v^i$可分别进行计算。

### 多头注意力

需要多个$q,k,v$。在计算时注意$q,k,v$要配对，$q_1$对$k_1,v_1$

### 位置编码

### 与CNN联系

CNN是自注意力机制的子集。



# 引用

[Self-attention 自注意力机制讲解 李宏毅版 v.s 吴恩达版 - 知乎](https://zhuanlan.zhihu.com/p/505105707)

